{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensional Reduction \n",
    "#### Part II\n",
    "Determining a metric by which to compare dimensionality reduction techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tSNE_utils # load utility functions from local file\n",
    "\n",
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore') # allow divide by zero for normalization\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "\n",
    "# scikit packages\n",
    "import skbio\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import maxabs_scale\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# import tsne packages\n",
    "from sklearn.manifold import TSNE\n",
    "import sys; sys.path.append('/Users/Cody/git/FIt-SNE')\n",
    "from fast_tsne import fast_tsne\n",
    "\n",
    "# plotting packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set(style = 'whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in data from three replicates (each containing 8 sets of 375 cells), and normalize them all a couple of different ways, perform PCA, and look at correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.48 s, sys: 1.27 s, total: 8.75 s\n",
      "Wall time: 9.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load the hdf5 files into dictionary\n",
    "reps = {\n",
    "'r00':tSNE_utils.read_hdf5('inputs/GSE102698ClosenessRep_0.hdf5'),\n",
    "'r01':tSNE_utils.read_hdf5('inputs/GSE102698ClosenessRep_1.hdf5'),\n",
    "'r02':tSNE_utils.read_hdf5('inputs/GSE102698ClosenessRep_2.hdf5')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to test two normalization methods:\n",
    "* Fractional read counts calculated as each gene count divided by the sum of all genes for each cell \n",
    "* `sklearn.preprocessing.normalize` designed for sparse dataframes. We'll just use default settings (`norm = 'l2'`) \n",
    "  \n",
    "And then transform the normalized matrices with two functions:\n",
    "* arcsinh(norm*1000)\n",
    "* log2(norm+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arcsinh_norm(norm, scale = 1000):\n",
    "    '''\n",
    "    Perform an arcsinh-transformation on a np.ndarray containing normalized data of shape=(n_cells,n_genes).\n",
    "    Useful for feeding into PCA or tSNE.\n",
    "        scale = factor to multiply values by before arcsinh-transform. \n",
    "        scales values away from [0,1] in order to make arcsinh more effective.\n",
    "    '''\n",
    "    return np.arcsinh(norm * scale)\n",
    "    \n",
    "def log2_norm(norm, frac = True):\n",
    "    '''\n",
    "    Perform a log2-transformation on a np.ndarray containing normalized data of shape=(n_cells,n_genes).\n",
    "    Useful for feeding into PCA or tSNE.\n",
    "    '''\n",
    "    return np.log2(norm + 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the 0.75 \"closeness\" replicate as an example, it looks like the log2 normalization of fractional read coverage has a closer __Wasserstein or \"Earth-Movers\" Distance__ than the arcsinh function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arcsinh EMD: 0.23876311381274876\n",
      "log2 EMD: 0.0006935577783836777\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r00_075 = np.nan_to_num(r00['Close_0.75'] / np.sum(r00['Close_0.75'], axis=0)) # get fractional counts\n",
    "\n",
    "r00_075_sinh = arcsinh_norm(r00_075) # arcsinh transform\n",
    "print('arcsinh EMD: {}'.format(sc.stats.wasserstein_distance(r00_075.flatten(), r00_075_sinh.flatten()))) # calculate EMD from normalized to normalized-transformed\n",
    "\n",
    "r00_075_log = log2_norm(r00_075) # log2 transform\n",
    "print('log2 EMD: {}\\n'.format(sc.stats.wasserstein_distance(r00_075.flatten(), r00_075_log.flatten()))) # calculate EMD from normalized to normalized-transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But I'm not sure if that's the best metric...  \n",
    "Let's look at the __correlation of distance matrices__ between the raw data and the two normalized/transformed matrices.  \n",
    "This will tell us how much each sample (or cell) in the matrix was moved relative to its neighbors following the transformation.  \n",
    "(a higher R value is better here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arcsinh Mantel R: 0.33728508594344886\n",
      "log2 Mantel R: 0.34900162855676653\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sinh_R,sinh_p,sinh_n = tSNE_utils.corr_distances(sc.spatial.distance_matrix(r00['Close_0.75'],r00['Close_0.75']),sc.spatial.distance_matrix(r00_075_sinh,r00_075_sinh),plot_out=False)\n",
    "log2_R,log2_p,log2_n =tSNE_utils.corr_distances(sc.spatial.distance_matrix(r00['Close_0.75'],r00['Close_0.75']),sc.spatial.distance_matrix(r00_075_log,r00_075_log), plot_out=False)\n",
    "\n",
    "print('arcsinh Mantel R: {}\\nlog2 Mantel R: {}\\n'.format(sinh_R, log2_R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the R value of the Mantel test comparing the cell-cell distances from the raw data to those in the transformed data is slightly higher for the log2-transformed counts as well. Seems compelling, but I want to test further.  \n",
    "  \n",
    "Let's do the same exercise with the `sklearn.preprocess.normalize()` function to see if it's better/different than manually-calculated fractional counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arcsinh EMD: 0.39737249207984604\n",
      "log2 EMD: 0.002872566345361386\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r00_075 = normalize(r00['Close_0.75'], axis=0, norm='l2') # normalize to square root of sum of squares for each cell\n",
    "\n",
    "r00_075_sinh = arcsinh_norm(r00_075) # arcsinh transform\n",
    "print('arcsinh EMD: {}'.format(sc.stats.wasserstein_distance(r00_075.flatten(), r00_075_sinh.flatten()))) # calculate EMD from normalized to normalized-transformed\n",
    "\n",
    "r00_075_log = log2_norm(r00_075) # log2 transform\n",
    "print('log2 EMD: {}\\n'.format(sc.stats.wasserstein_distance(r00_075.flatten(), r00_075_log.flatten()))) # calculate EMD from normalized to normalized-transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These transformed values are a bit further via EMD from the normalized matrix. No sweat, let's look at the correlation of the distance matrices to the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arcsinh Mantel R: 0.34263435505647516\n",
      "log2 Mantel R: 0.3390754271795316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sinh_R,sinh_p,sinh_n = tSNE_utils.corr_distances(sc.spatial.distance_matrix(r00['Close_0.75'],r00['Close_0.75']),sc.spatial.distance_matrix(r00_075_sinh,r00_075_sinh),plot_out=False)\n",
    "log2_R,log2_p,log2_n =tSNE_utils.corr_distances(sc.spatial.distance_matrix(r00['Close_0.75'],r00['Close_0.75']),sc.spatial.distance_matrix(r00_075_log,r00_075_log), plot_out=False)\n",
    "\n",
    "print('arcsinh Mantel R: {}\\nlog2 Mantel R: {}\\n'.format(sinh_R, log2_R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-oh, the better fit via Mantel test flipped.  \n",
    "This tells me that these normalization methods may be identical in terms of utility, or maybe it's just this one dataset that's ambiguous..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate df for dumping correlation data into\n",
    "corr_out = pd.DataFrame()\n",
    "\n",
    "for rep in reps.keys():\n",
    "    for key in reps[rep].keys():\n",
    "        \n",
    "        frac = np.nan_to_num(reps[rep][key] / np.sum(reps[rep][key], axis=0)) # get fractional counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wasserstein Distances:\n",
      "\n",
      "\n",
      "Normalize \"l2\": 0.0016846641844954862\n",
      "Maxabs Scaled: 0.026490231334317764\n"
     ]
    }
   ],
   "source": [
    "d1 = sc.stats.wasserstein_distance(test1.flatten(), test2.flatten())\n",
    "d2 = sc.stats.wasserstein_distance(test1.flatten(), test3.flatten())\n",
    "\n",
    "print('Wasserstein Distances:\\n\\n\\nNormalize \"l2\": {}\\nMaxabs Scaled: {}'.format(d1,d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
